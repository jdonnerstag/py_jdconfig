# ----------------------------------------------------------------------------
# PIL Engine config file
# ----------------------------------------------------------------------------

fn_CENT:
    full: "{ref: data_files_input_dir}/ANO_DWH..DWH_TO_PIL_CENT_{arg:fname}_(VTAG|FULL).*.A901"
    delta: "{ref: data_files_input_dir}/ANO_DWH..DWH_TO_PIL_CENT_{arg:fname}_DELTA.*.A901"

PROJECT_ROOT: C:\\source_code\\COMSAFE

# Number of processes used to load multiple files
# Use a negative number to dynamically determine the pool size based on server
# CPU, core and threads sizes. E.g. -1 => leave 1 core to the OS
# TODO: Keep for now as example, even though we are not using it right now.
process_pool: -1

file_configs:
    git_repo: "{ref:PROJECT_ROOT}\\PIL-config-2"
    # If "", then use 'config_env_key'
    # if None, then don't checkout branch
    branch: None     # "master", {ref:conf_env_key}
    pull_on_init: False  # True,
    directory: "file_config"

# Defaults. Might be overriden by specific file configs
files:
    # Can be overriden in the file config
    default_encoding: "iso_8859_1"

    # Default Root-Directory for output data
    # See pil_flow.py -d command line option
    PIL_DATA: "{ref:PROJECT_ROOT}\\PIL-data-2"

    # The log file containing the details of a specific file import
    # Note that this config requires user provided values, not included in the config
    # Note: timestamp hsa been defined as lambda function and hence the return value is dyanmic
    log_file: "{ref:log_dir}/{ref:filename}.{ref:timestamp}.log"

    # If not provided via the commandline
    input_files: "c:\\temp\\PIL\\anon_test_data\\*.gz"

    # The directory where sorted input files (VTAG, FULL, DELTA) are searched for by default
    input_files_sorted: "{ref:PROJECT_ROOT}\\VF-data\\12_sorted"

    # The directory where FULL files are written to, after applying DELTA files
    delta_files_applied: "{ref:PROJECT_ROOT}\\VF-data\\15_deltas_applied"

    com_periods: "{ref:PROJECT_ROOT}\\VF-data\\16_com_periods"

    step1_output: "{ref:PROJECT_ROOT}\\PIL-data-2\\11_input_reviewed"

    # The file where errornous records will be appended
    failed_records: "{ref:log_dir}/{ref:filename}.{ref:timestamp}.failed"

    # The file where filtered records will be appended
    filtered_records: "{ref:log_dir}/{ref:filename}.{ref:timestamp}.filtered"

    # Rename or move the file before processing
    rename_when_processing:      # "{filedir}/{filename}.{timestamp}.processing",

    # Rename or move the file after processing
    rename_when_finished:        # "{filedir}/{filename}.{timestamp}.done",

    # Events that are filtered during pipeline processing
    event_filtered: "{ref:log_dir}/events_filtered.{ref:timestamp}.log"

    # Permanent FN license database file
    FN-license-db: "{ref:embedded-db.path}/FN-licenses.pickle"

    MICOS-DB-File: "{ref:embedded-db.path}/MICOS-DB-File.pickle"


# Data-files provided in S3 buckets, via SFTP, or similar. They are potentially large.
data_files_input_dir: "{ref:PROJECT_ROOT}/PIL-data-2/11_input_crlf_fixed"
data_files:
    assignment-full:
        - "{ref:data_files_input_dir}/ANO_DWH..DWH_TO_PIL_ASSIGNMENT_VTAG.*.A901"
        - "{ref:data_files_input_dir}/ANO_DWH..DWH_TO_PIL_ASSIGNMENT_FULL.*.A901"

    assignment-delta":
        - "{ref:data_files_input_dir}/ANO_DWH..DWH_TO_PIL_ASSIGNMENT_DELTA.*.A901"

    CENT:
        customer_account: {call: fn_CENT, fname, "CUSTOMER_ACCOUNT"}
        named_account_assign: {call: fn_CENT, fname, "NAMED_ACCOUNT_ASSIGN"}
        party: {call: fn_CENT, fname, "PARTY"}
        sales_assignment: {call: fn_CENT fname, "SALES_ASSIGNMENT"}
        SL_main_resp: {call: fn_CENT, fname, "SL_MAIN_RESP"}

# The list of reference data files that must be loaded before event processing
pil_input_files: "{ref:git.manual_files_git_repo}/input_files.xlsx"
